\documentclass{article}
\usepackage[a4paper, left=1.5in, right=1.5in, top=1.5in, bottom=1.5in]{geometry}

% \usepackage[square,numbers]{natbib}
\usepackage{biblatex}
\addbibresource{references.bib}

\usepackage{graphicx} % Required for inserting images
\usepackage{url}

\title{Technical, Ethical, and Legal Considerations Regarding the Implementation of Facial Recognition in Shopping Centres}
\author{Oran Keating}
\date{December 2024}

\begin{document}

\maketitle

\newpage


\addtocontents{toc}{\setcounter{tocdepth}{2}}
\tableofcontents
\newpage
\begin{abstract}
    With the integration of facial recognition technology into public spaces growing more prevalent, it is becoming increasingly important to critically examine the ethical implications of its use rather than treat it as a trivial addition to the current pool of surveillance technology to ensure its continued integration into society remains ethical. This research evaluates FRT's potential effectiveness in shopping malls as a method of footfall monitoring and enforcing banning orders, as well as highlighting key considerations for its responsible implementation by examining the legal and ethical implications of its use. This report aims to guide the council's decision-making by outlining key considerations that should be made to ensure an informed and responsible approach to FRT implementation. This report highlights the necessity of carefully balancing the benefits of the technology with its ethical challenges. 
\end{abstract}
\newpage
\section{Introduction}
Facial recognition technology (FRT) is “technology that can detect and extract a human face from a digital image then match this face against a database of pre-identified faces”\cite{Selwyn_Andrejevic_O’Neill_Gu_Smith_2024}. FRT is becoming increasingly ubiquitous, with three-quarters of governments around the world using it on a large-scale basis, three countries in the world having no evidence of use, and two having outright banned it \cite{Bischoff2021}. FRT is used most commonly in police forces, airports, and banking institutions. While its use in these institutions highlights a common theme of adoption in environments where security and verification of identities are important, how it is implemented and employed varies from institution to institution and its specific use case within \cite{Bischoff2021}.

Currently, three distinct types of FRT are being used \cite{Selwyn_Andrejevic_O’Neill_Gu_Smith_2024}. One-to-one compares a single face to a set of stored faces. This type is often used for authentication, such as FaceID being used to unlock phones, integration into the IoT using FRT to lock or unlock doors \cite{FaceIdDoors}
and its implementation into ePassports at the UK border to allow for increased throughput \cite{BorderControl}.

One-to-many compares one face against a database of other faces looking for a match \cite{Selwyn_Andrejevic_O’Neill_Gu_Smith_2024}. This type is currently being used to help solve real-world criminal cases by correctly identifying suspects in surveillance footage \cite{Zhang2018}, identifying patients in hospitals \cite{Sadahide2024}, \cite{FaceIDpatient}, and could eventually be used to identify non-verbal patients in need of medical attention. This type will most likely be used to identify those violating banning orders. 

The final type is inference. Its purpose is to infer attributes about a person, whether that be physical, such as race and gender, or emotional states, such as happiness or excitement \cite{Selwyn_Andrejevic_O’Neill_Gu_Smith_2024}. (current use/future use) This type could be used in footfall monitoring to detect the kind of person most commonly found in a certain area and/or their emotional reaction to a certain stimulus, such as seeing a specific shop or product. 

Due to FRT being a constantly evolving technology, the debate surrounding its legal and ethical implications is widespread and ever-changing. There is a concern that FRT, when misused, will encroach on people's privacy. Some argue that in this digital age, privacy is virtually impossible as we “live in a world in which almost everything we do, everywhere we go, and everything we buy, is recordable” \cite{Marmor2023} and that that data is more often than not “aggregated, sorted, and often sold” \cite{Marmor2023} and then used against us in an attempt to profit \cite{SurveillanceCapitalism} , often violating people's rights in doing so \cite{Richards2013}. 

Others argue that privacy is rapidly degrading due to misuse, suggesting that “spyware tools have often been used for illegitimate reasons, including to clamp down on critical or dissenting views and on those who express them, including journalists, opposition political figures and human rights defenders” and that most phones can easily be turned into “24-hour surveillance devices” \cite{UNRightToPrivacy}. As algorithms become increasingly good at inferring facts from seemingly unimportant data \cite{Ballesteros2024}, \cite{Coe2021} FRT has the potential to rapidly diminish what privacy we have left due to the sensitivity of the data it collects \cite{ICO} and the inferences it can make.

Some are enthusiastic and excited by the individual and societal conveniences that FRT could bring and make suggestions as to where future areas of research should focus \cite{PPFOfFRT} with some choosing to demonstrate the failings of current privacy law, highlighting the need for new laws and legislation \cite{Goldenfein_2024}.

Implementing FRT in Southampton shopping centres could result in huge benefits such as increased public safety and benefits to urban development due to the footfall data collected. The problem is how FRT can be implemented effectively without encroaching on people's privacy. Failing to properly consider the challenges involved in its implementation could result in lawsuits \cite{Loideain_2024}, violating ethical standards, and a loss of public trust in the authorities.

Due to the numerous differing opinions on how to best implement FRT, maintaining its effectiveness while still respecting privacy, a wide range of factors from all areas must be considered, making the scale of the problem huge. As this report focuses on a specific use case, the scale of the problem diminishes to only those factors most relevant to its desired implementation. It will focus specifically on the second and third types of FRT: one-to-many matching for enforcing banning orders and inference-based systems for footfall monitoring.
 
The rapid advancement of FRT and its increasing ubiquity necessitate an ongoing critical examination of the legal and ethical implications of its use. Given the numerous concerns around FRT's use, it is crucial to proactively address these issues so the laws and ethical guidelines we hold ourselves to regarding its implementation do not fall behind its rapid progress. Ensuring FRT is consistently used responsibly is necessary for its continued integration into society.




\section{Technical affordances and limitations}
FRT has rapidly advanced thanks to its integration with deep learning technology and artificial intelligence \cite{PPFOfFRT}. This combination has improved FRT's performance to such an extent that it is seeing real-world use cases in police infestations and border control. However, for all of this to be successful, it must be able to perform its function effectively within the context it is applied.


\subsection{The importance of training data}

Much of the success of FRT is down to the data it is trained on. A good database not only increases FRT's success rate but also allows academics to reliably test and evaluate their algorithms and methods of implementation \cite{PPFOfFRT}. A lack of variety of datasets could result in racial discrimination, which could eventually lead law enforcement to specifically target marginalised populations \cite{RacialDiscrimination}. It is clear that diverse and varied datasets are required both to prevent racial discrimination and to improve the success rate of FRT. This is especially important when implementing FRT in shopping centres, as the conditions are not easily controllable. This requires the data set to also have a wide range “of faces shown at different angles and exposed with
different lighting conditions” \cite{CDEI2024}. The algorithms that FRT uses can either be brought pre-trained or not. While purchasing a pre-trained algorithm “may limit transparency into how these algorithms are designed and the data they are trained on” \cite{CDEI2024}, training is incredibly resource intensive. For example, “Google used a dataset of 200 million facial images over four weeks to train an FRT system in 2016” \cite{CDEI2024}. Most organisations choose to not train the algorithms themselves due to the intensive computing power and amount of resources required, and while it does limit transparency, often it is not feasible. In addition, there is a risk that, if not using a pre-trained algorithm, the datasets are incorrect, missing data, or not varied enough. Whether the algorithm is pre-trained or not, it is vital that the datasets used “(i) contain a large number of persons and photographs, (ii) follow real-world requirements, and (iii) are open to the public” \cite{PPFOfFRT}. This will only be a concern if FRT is not brought pre-trained, as companies training it are most likely going to use a variety of correct datasets.

\subsection{Current Limitations and Proposed Solution}
As far as FRT has come, there are still limitations that impact its effectiveness and reliability. The limitations this report will be talking about are specific to its use case in shopping malls. While other limitations exist, they fall outside the scope of this report.

The first challenge is facial occlusions. Physical items, such as masks, glasses, or scarves; environmental factors, such as bright lights, shadows, or reflections; and positional challenges, such as facial orientation, all contribute to low recognition accuracy \cite{PPFOfFRT}. Environmental and positional challenges can be mitigated through testing and strategic camera placement, respectively. For example, eye-level cameras have demonstrated improved performance \cite{Bacci2021}. However, physical occlusions are much harder to address, requiring advancements in processing techniques instead of adjustments to the hardware and environment. If aware, offenders could exploit it by deliberately obscuring their faces. One possible way of limiting the impact of physical facial occlusions is to include images in the training data. The research in \cite{PPFOfFRT} suggests making use of the ‘AR Dataset,’ which includes over 3000 images of subjects with varying physical facial occlusions under various illumination conditions. For footfall monitoring, the impact of facial occlusions varies depending on the type of data you are trying to collect. For example, if people's individual reactions to specific shops are part of the desired data, facial occlusions will have a massive impact. However, if the desired data does not include anything related to people's faces, then the impact will be reduced; in this scenario you might consider if FRT is even necessary.

Another challenge is that of aging. As the gap between the reference image and an individual's current appearance grows, the accuracy of facial recognition decreases \cite{PPFOfFRT}. While this may not pose an issue when FRT is used for footfall monitoring, it is needed to identify those who violate banning orders, as accurately matching a face is required. One way to address this issue is to maintain an updated database of reference images by taking a new photo of offenders each year. This approach, while a valid solution, will require sustained use of resources, as the offender will need to be taken in each year.

The final challenge is that FRTs performance is mostly reliant on the conditions under which the image is acquired. As previously discussed, facial occlusions have an impact on performance, but so does the image quality. 

For any type of FRT, a clear image of sufficient quality is needed to achieve reliable and accurate results \cite{Bacci2021}. Image quality relies on both the quality of the hardware and also “lighting conditions, angle of incidence, SCD, distortions, color, visibility of features, and more.” \cite{Bacci2021} If the image is not of sufficient quality, the challenge of FRT significantly increases. Images with low resolutions, block artefacts, or a blur can obscure facial features or hinder the system's ability to distinguish between multiple individuals, particularly in crowded environments like shopping centers. In this context a high-quality image is necessary for FRT to function effectively \cite{PPFOfFRT}. 

To improve image quality, factors on both the hardware and software sides must be taken into account. For example, if integrating FRT directly with existing CCTV image pre-processing, it may be necessary due to CCTVs use of lossy compression potentially distorting the image \cite{PPFOfFRT}. Obtaining the required clear image is oftentimes much more difficult than expected. According to research conducted by \cite{Bacci2021}: “controlling for or identifying which of the multiple limiting factors contributed to the poor performance of MA would be impossible,” and that even in sufficient conditions the “limiting factors cannot be isolated from one another,” making it very hard to determine what is actually causing a poorer quality image. They conclude that the best way to improve the image quality, both in research and practical applications, is to have a multidisciplinary approach "with involvement from the fields of anatomy, forensic anthropology, photography, image science, and psychology, among others." 

\section{Ethics and the Law}
\subsection{The unique sensitivity of facial data}
Unlike other forms of biometric data, the face is “deeply linked to personal, social, and institutional identities” \cite{EthicsOfFRTOxford}, making the ethical issues surrounding FRT uniquely challenging. Viewing someone's face can subconsciously influence decisions that should be driven solely by objective information, such as “criminal justice decisions” or “congressional elections” \cite{Zebrowitz2008}. It is one of the most powerful and utilised tools in exchanging information and non-verbal communication, allowing observers to quickly and easily, and often involuntarily, make a number of inferences about a person, whether you want them to or not \cite{JACK2015R621}, \cite{Hall2019}. The huge amount of diverse information a face reveals about a person makes having one's face scrutinised a deeply personal act, and as such, the idea that a machine might do this automatically and the uncertainty of how the data captured is used makes FRT a highly contentious topic. Even if the face is not seen as personal or sensitive data, when used in combination with other information, sensitive data can be extracted through inference \cite{SurveillanceDelusionOxford}. \cite{SurveillanceDelusionOxford} notes this is already seen in companies "inferring sexual orientation from music preferences".

Due to all these factors, the ethical decisions made around the implementation of FRT are unique in their complexity and therefore must be given adequate consideration. The General Data Protection Regulation (GDPR) act recognises these complexities and challenges by classifying data to do with the face as ‘sensitive data,’ meaning it is prohibited to process unless an exception applies \cite{KPMG_GDPR}.

\subsection{The ethics and practicalities of consent}
One of the key exceptions under the GDPR that permits the processing of sensitive facial data is consent. Consent is a central ethical and legal consideration in regards to FRT, especially given the sensitivity of the data being collected. The ability to give informed consent is becoming increasingly difficult as more and more data is being collected about people and shared between various companies legally and illicitly \cite{SurveillanceDelusionOxford}. One example is the Cambridge Analytica scandal, where data was collected from millions of Facebook users without their consent and used to influence elections \cite{MATZ2020116}. This problem is exacerbated if the implementation of FRT is outsourced to tech giants where there is a lack of control over how the data will be processed or used. While there is no problem with outsourcing the technology, there can be no guarantee to the public that their personal data won't be used illicitly \cite{SurveillanceDelusionOxford}. In addition, informed consent may be impossible, as with FRT having the ability to infer and draw conclusions from received data \cite{Supervisor2021} “neither the people who wrote the privacy policy nor the programmers who wrote the code for the algorithms that analyse data know what kind of inferences might be drawn” \cite{SurveillanceDelusionOxford}. 

As important as the ethical concerns about consent are, it is equally important to consider the practical challenges of how consent will be obtained in a real-world setting. When FRT is used in the context of authentication, such as in FaceID, consent can be easily given; however, it becomes more challenging when implemented in public spaces. \cite{IOC_LiveFRT} found that in circumstances where FRT is implemented in public places, it is unlikely that valid consent will be able to be collected for all individuals whose data is processed, and if it is given, that it won't be “freely given, specific, informed and unambiguous”. All these challenges suggest that consent may not be appropriate for these circumstances and that “asking for consent is misleading and inherently unfair” \cite{IOC_LiveFRT}. Having concluded that obtaining consent will be practically challenging and ethically problematic, it follows to look at other avenues where FRT can be implemented without the need of explicit consent.

Legal frameworks recognise the challenges of obtaining proper consent in these situations and, under specific circumstances, allow FRT to be used without consent being given. Under Article 6 of the GDPR, FRT can be used if the task it is performing is ‘carried out in the public interest or in the exercise of official authority vested in the controller’ and that the task has a basis in law. It must also be demonstrated that FRT is ‘a necessary and
proportionate means of fulfilling the obligation or task’ \cite{IOC_LiveFRT}. For detailed guidance on how to ensure compliance with these legal requirements, resources such as \cite{IOC_LiveFRT} exist. Implementing FRT in this way is a sensible option as it ensures complete legal compliance and also deals with a lot of the ethical problems surrounding consent, as provided its use is necessary and proportional, not asking for explicit consent is justifiable.

\subsection{Necessity and Proportionality}
From both an ethical and legal perspective, facial recognition must be necessary and proportional to justify its use \cite{SurveillanceDelusionOxford}. The Investigatory Powers Act of 2016 explains that proportionality must be taken into account by public authorities when making decisions that affect privacy \cite{Investigatory_Powers_Act}. Similarly, Article 8 of the European Convention on Human Rights (ECHR) states that public authorities can only interfere with the right to privacy if the action is "lawful, necessary and proportionate" \cite{Article8}. Employing FRT where it may not be necessary or proportional is not only ethically irresponsible but could result in legal challenges such as in the case of “Bridges v. South Wales Police,” where it was determined that the police use of FRT “was ‘not in accordance with the law’ and therefore in breach of Article 8 ECHR.” Due to all this, extra care must be given to considering FRT's necessity and proportionality \cite{Loideain_2024}. In addition, after it is employed, as the laws are constantly being updated, it must be frequently reviewed to ensure it continues to comply.

When assessing necessity, it should be taken into consideration whether the frequency of banning order violations justifies the use of FRT or if the current methods of enforcement are sufficient. Similarly, with footfall monitoring, looking at the effectiveness of current footfall monitoring methods is necessary. Considering necessity requires exploring existing solutions that achieve the same goals as FRT with less of an impact on privacy. For example, in regards to those breaking banning orders, GPS tags have been used previously to “hunt burglars and cut theft” \cite{GPS_Tags}, which could similarly be used to detect when people violate banning orders. In regards to footfall monitoring, less invasive methods exist, such as CCTV, electronic sensors, mobile foot printing, and kinetic pavements, which can all provide sufficient data. 

When considering proportionality in its use, sensible placement of cameras could help mitigate its impact on privacy. For example, depending on what footfall data needs to be collected, only certain cameras might use FRT, such as the cameras pointing at entrances and exits to the shopping centre. 

\subsection{Unintended Consequences}

The unintended consequences should also be taken into account when considering using FRT. If transparency is not employed when using FRT, the general public's trust in authority may diminish. Even if transparency is employed because it is such new technology and therefore not perfect, people may feel “their ability to be treated fairly, succeed on their own merits, and receive equal justice” \cite{EthicsOfFRTOxford}. Alternatively, if it is ever perfected and “individuals believe they have no escape from the ubiquity of surveillance, they will be even more likely to lose trust in assurances that their data and privacy choices are being respected or enforced” \cite{EthicsOfFRTOxford}. This could result in people actively avoiding the area FRT is used, creating the opposite effect than intended. The best way to mitigate this is by being completely transparent about how and why FRT is being implemented and how the collected data is being processed.

\section{Conclusion}

In conclusion, the integration of FRT into Southampton shopping centres has the potential to be highly effective. Its effectiveness relies on sufficient consideration of technical, legal, and ethical challenges. As demonstrated, addressing technical challenges requires a diverse team with a variety of skills, and similarly addressing ethical challenges also necessitates considering a range of viewpoints. This report concludes that due to the deeply personal and sensitive nature of the data FRT collects, extra care must be taken when considering all factors. It also concludes that logistically, consent from each individual person is too impractical to achieve; however, the law recognises this and provides adequate solutions. From a hardware perspective, it is possible to integrate FRT with existing CCTV systems in shopping centres; however, to make it as effective as possible, a multifaceted approach must be taken to its implementation. From a software side, purchasing a pre-trained algorithm is the most sensible approach for this situation, as the resources and time required to train one will most likely outweigh the benefits it brings. This report recommends that all challenges should be addressed in depth, prioritising legal compliance first, especially assessing its necessity and proportionality. This is because if it is concluded that FRT is not necessary and proportional, no other factors need to be considered. It is vital to consider pre-existing solutions rather than adamantly stick to the use of FRT. While this report tries to discuss as many relevant factors as possible, there are still technical, ethical, and legal challenges that should be looked at in detail that go unmentioned in this report. 

\printbibliography

\end{document}
